# -*- coding: utf-8 -*-
"""TF_Compara√ß√£o_entre_modelos_CNNs_VJ_082025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kTJ5-9zCZnHB1HFto1N5tcP4iZe2ouqb
"""

# Commented out IPython magic to ensure Python compatibility.
# General Libs
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.optimizers import Adam
import numpy as np
import random
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import drive
# Montar o Google Drive
drive.mount('/content/drive')

im_shape = (250,250)

#TRAINING_DIR = '/content/drive/MyDrive/chest_xray_pequeno/train'
#TEST_DIR = '/content/drive/MyDrive/chest_xray_pequeno/test'
TRAINING_DIR = '/content/drive/MyDrive/chest_xray/train'
TEST_DIR = '/content/drive/MyDrive/chest_xray/test'
seed = 10

BATCH_SIZE = 16

TRAINING_DIR

from google.colab import drive
import matplotlib.pyplot as plt
import cv2

# Montar o Google Drive
#drive.mount('/content/drive')

# Caminho da imagem
#image_path = "/content/drive/MyDrive/chest_xray_pequeno/test/person1_virus_11.jpeg"
image_path = "/content/drive/MyDrive/chest_xray/test/NORMAL/IM-0001-0001.jpeg"

# Carregar a imagem em escala de cinza (opcional)
image = cv2.imread(image_path)  # Para RGB use cv2.IMREAD_COLOR
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convertendo para RGB

# Exibir a imagem
plt.figure(figsize=(5, 5))
plt.imshow(image)
plt.axis('off')  # Remove os eixos
plt.show()

TEST_DIR

#Using keras ImageGenerator and flow_from_directoty

# Subdivision in test/validation

# Without data augmentation
data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.2)

# If you want data augmentation, uncomment and run the following
## data_generator = ImageDataGenerator(
##         validation_split=0.2,
##         rotation_range=20,
##         width_shift_range=0.2,
##         height_shift_range=0.2,
##         rescale=1./255,
##         shear_range=0.2,
##         zoom_range=0.2,
##         horizontal_flip=True,
##         fill_mode='nearest')

val_data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.2)

data_generator

val_data_generator

from google.colab import drive

# Montar o Google Drive
drive.mount('/content/drive')

# Tamanho da imagem
im_shape = (250, 250)  # Ajuste conforme necess√°rio
BATCH_SIZE = 32
seed = 42

# Generator para parte train
train_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=im_shape, shuffle=True, seed=seed,
                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset="training")
# Generator para parte valida√ß√£o
validation_generator = val_data_generator.flow_from_directory(TRAINING_DIR, target_size=im_shape, shuffle=False, seed=seed,
                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset="validation")

# Generator para dataset de teste
test_generator = ImageDataGenerator(rescale=1./255)
test_generator = test_generator.flow_from_directory(TEST_DIR, target_size=im_shape, shuffle=False, seed=seed,
                                                     class_mode='categorical', batch_size=BATCH_SIZE)

nb_train_samples = train_generator.samples
nb_validation_samples = validation_generator.samples
nb_test_samples = test_generator.samples
classes = list(train_generator.class_indices.keys())
print('Classes: '+str(classes))
num_classes  = len(classes)

print(f"Total de imagens de treino: {nb_train_samples}")
print(f"Total de imagens de valida√ß√£o: {nb_validation_samples}")
print(f"Total de imagens de teste: {nb_test_samples}")

import os
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from google.colab import drive

# Montar o Google Drive
drive.mount('/content/drive')

# Definir os diret√≥rios
#TRAINING_DIR = "/content/drive/MyDrive/chest_xray/train"
#TEST_DIR = "/content/drive/MyDrive/chest_xray/test"

# Tamanho da imagem
im_shape = (250, 250)  # Ajuste conforme necess√°rio
BATCH_SIZE = 32
seed = 42

# Fun√ß√£o para pegar 10% dos arquivos em cada classe
def get_limited_data(directory, fraction=0.05):
    all_classes = os.listdir(directory)
    sampled_files = []

    for class_name in all_classes:
        class_path = os.path.join(directory, class_name)
        if os.path.isdir(class_path):
            files = np.array(os.listdir(class_path))  # Lista os arquivos
            np.random.shuffle(files)  # Embaralha os arquivos
            sample_size = max(1, int(len(files) * fraction))  # Calcula 10% (m√≠nimo de 1)
            sampled_files.extend([(class_name, f) for f in files[:sample_size]])

    return sampled_files

# Criar gerador de imagens para treino e valida√ß√£o
train_data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # 80% treino, 20% valida√ß√£o

train_generator = train_data_gen.flow_from_directory(
    TRAINING_DIR, target_size=im_shape, batch_size=BATCH_SIZE,
    class_mode='categorical', shuffle=True, seed=seed, subset="training")

validation_generator = train_data_gen.flow_from_directory(
    TRAINING_DIR, target_size=im_shape, batch_size=BATCH_SIZE,
    class_mode='categorical', shuffle=False, seed=seed, subset="validation")

# Criar gerador de imagens para teste com 10% dos arquivos
test_files = get_limited_data(TEST_DIR, fraction=0.05)
test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(
    TEST_DIR, target_size=im_shape, batch_size=BATCH_SIZE,
    class_mode='categorical', shuffle=False, seed=seed)

# Ajustar o n√∫mero de amostras
nb_train_samples = int(train_generator.samples * 0.05)
nb_validation_samples = int(validation_generator.samples * 0.05)
nb_test_samples = len(test_files)

classes = list(train_generator.class_indices.keys())
print('Classes:', classes)
num_classes = len(classes)

print(f"Total de imagens de treino: {nb_train_samples}")
print(f"Total de imagens de valida√ß√£o: {nb_validation_samples}")
print(f"Total de imagens de teste: {nb_test_samples}")

train_generator

test_generator

import matplotlib.pyplot as plt

# Visualizando algumas imagens do gerador de treinamento
plt.figure(figsize=(15, 15))

for i in range(9):
    plt.subplot(330 + 1 + i)

    # Obt√©m um batch de imagens e seleciona a primeira imagem do lote
    batch = next(train_generator)[0] * 255
    image = batch[0].astype('uint8')  # Pegando apenas a primeira imagem do batch

    plt.imshow(image)
    plt.axis('off')  # Remove os eixos para melhor visualiza√ß√£o

plt.show()

"""# **Modelo keras.applications DenseNet121**

"""

from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# Carregar o modelo pr√©-treinado sem a √∫ltima camada
base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(250, 250, 3))

# Criar um novo modelo com camadas extras
model = Sequential([
    base_model,
    Flatten(),
    Dense(256, activation='relu'),
    #Dense(1, activation='sigmoid')  # Para classifica√ß√£o bin√°ria
    Dense(2, activation='softmax')  # Para classifica√ß√£o multiclasse
])

# Congelar as camadas pr√©-treinadas
for layer in base_model.layers:
    layer.trainable = False

# Compilar
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

#epochs = 80 xxxx
epochs = 10


from tensorflow import keras
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

#Callback to save the best model
callbacks_list = [
    keras.callbacks.ModelCheckpoint(
        filepath='modelvgg16.h5',
        monitor='val_loss', save_best_only=True, verbose=1),
    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,verbose=1)
]


# ‚úÖ **CORRE√á√ÉO**: Ajuste `class_mode="binary"` para corresponder √† sa√≠da do modelo
train_generator = data_generator.flow_from_directory(
    TRAINING_DIR,
    target_size=(250, 250),
    shuffle=True,
    seed=seed,
    class_mode='binary',  # <- Importante: Sa√≠da deve ser bin√°ria
    batch_size=BATCH_SIZE,
    subset="training"
)

validation_generator = val_data_generator.flow_from_directory(
    TRAINING_DIR,
    target_size=(250, 250),
    shuffle=False,
    seed=seed,
    class_mode='binary',  # <- Importante: Sa√≠da deve ser bin√°ria
    batch_size=BATCH_SIZE,
    subset="validation"
)

# üöÄ **Treinamento**
history = model.fit(
    train_generator,
    steps_per_epoch=nb_train_samples // BATCH_SIZE,
    epochs=epochs,
    callbacks=callbacks_list,
    validation_data=validation_generator,
   validation_steps=nb_validation_samples // BATCH_SIZE,
    verbose=1
)


#Training
## history = model.fit(
##         train_generator,
##         steps_per_epoch=nb_train_samples // BATCH_SIZE,
##         epochs=epochs,
##         callbacks = callbacks_list,
##         validation_data=validation_generator,
##         verbose = 1,
##         validation_steps=nb_validation_samples // BATCH_SIZE)

#Desempenho nos dados de treino
#loss - Erro. Ideal √© chegar em 0
#accuracy - % de acerto. Ideal √© chegar em 100%

#Desempenho nos dados de valida√ß√£o
#val_loss
#val_accuracy

from tensorflow import keras

# Definindo o n√∫mero de √©pocas e o batch size
epochs = 10

# Callback para salvar o melhor modelo
callbacks_list = [
    keras.callbacks.ModelCheckpoint(
        filepath='model_densenet121.h5',  # üîÑ Nome atualizado
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        verbose=1
    )
]

# üöÄ Treinamento
history = model.fit(
    train_generator,  # Gerador de dados de treino
    steps_per_epoch=nb_train_samples // BATCH_SIZE,
    epochs=epochs,
    callbacks=callbacks_list,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // BATCH_SIZE,
    verbose=1
)

# Training curves
import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

epochs_x = range(1, len(loss_values) + 1)
plt.figure(figsize=(10,10))
plt.subplot(2,1,1)
plt.plot(epochs_x, loss_values, 'bo', label='Training loss')
plt.plot(epochs_x, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation Loss and Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.subplot(2,1,2)
acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']
plt.plot(epochs_x, acc_values, 'bo', label='Training acc')
plt.plot(epochs_x, val_acc_values, 'b', label='Validation acc')
#plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend()
plt.show()

# Load the best saved model
from tensorflow.keras.models import load_model

model = load_model('model_densenet121.h5')

# ‚úÖ Avaliando com o dataset de valida√ß√£o
score = model.evaluate(validation_generator)
print('Val loss:', score[0])
print('Val accuracy:', score[1])

# ‚úÖ Avaliando com o dataset de teste
# Using the test dataset
### score = model.evaluate(test_generator)
### print('Test loss:', score[0])
### print('Test accuracy:', score[1])

# Generator para teste
# test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(
#     TEST_DIR,
#     target_size=im_shape,
#     batch_size=BATCH_SIZE,
#     class_mode='binary',
#     shuffle=False,
#     seed=seed
# )


score = model.evaluate(test_generator)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# Defina o n√∫mero de amostras de teste e o tamanho do lote
# BATCH_SIZE = 16  # Ajuste conforme necess√°rio
# # nb_test_samples = test_generator.samples  # N√∫mero total de amostras no conjunto de teste
# # steps_test = nb_test_samples // BATCH_SIZE  # N√∫mero de steps para o teste

# Usando o dataset de teste
# score = model.evaluate(test_generator, steps=steps_test, verbose=1)

# Exibindo as m√©tricas
# print('Test loss:', score[0])
# print('Test accuracy:', score[1])

import numpy as np
from sklearn.metrics import accuracy_score

# Obter as previs√µes
predictions = model.predict(test_generator, verbose=1)

# Obter os r√≥tulos reais
true_labels = test_generator.classes

# Calcular a acur√°cia
accuracy = accuracy_score(true_labels, np.argmax(predictions, axis=1))

print(f"Test Accuracy: {accuracy}")

import numpy as np
from sklearn.metrics import accuracy_score, roc_auc_score

# üîπ Previs√µes do modelo (probabilidades)
predictions = model.predict(test_generator, verbose=1)

# üîπ R√≥tulos reais (inteiros)
true_labels = test_generator.classes

# üîπ Calcular acur√°cia (usando classe com maior probabilidade)
accuracy = accuracy_score(true_labels, np.argmax(predictions, axis=1))
print(f"Test Accuracy: {accuracy:.4f}")

# üîπ Calcular AUC
# Para 2 classes, usa-se multi_class='ovr'
auc = roc_auc_score(true_labels, predictions[:,1], multi_class='ovr')
print(f"Test AUC: {auc:.4f}")

import tensorflow as tf
print(tf.__version__)

import itertools

#Plot the confusion matrix. Set Normalize = True/False
def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.figure(figsize=(10,10))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        cm = np.around(cm, decimals=2)
        cm[np.isnan(cm)] = 0.0
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Some reports
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# On test dataset
Y_pred = model.predict(test_generator)
y_pred = np.argmax(Y_pred, axis=1)
target_names = classes

# Confusion Matrix
cm = confusion_matrix(test_generator.classes, y_pred)
plot_confusion_matrix(cm, target_names, normalize=False, title='Confusion Matrix')

# Classification Report
print('Classification Report')
print(classification_report(test_generator.classes, y_pred, target_names=target_names))



print(model.input_shape)

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt

# Carrega o modelo treinado
model = load_model('model_densenet121.h5')

# Dicion√°rio de classes
class_dict = {0: "Normal", 1: "Pneumonia"}  # Ajuste conforme necess√°rio

def preprocess_image(img_path, target_size=(250, 250)):  # Alterado para 250x250
    img = image.load_img(img_path, target_size=target_size)
    img_array = image.img_to_array(img) / 255.0  # Normaliza√ß√£o
    img_array = np.expand_dims(img_array, axis=0)  # Adiciona dimens√£o do batch
    return img_array

def predict_image(img_path):
    """Classifica a imagem carregada."""
    img_array = preprocess_image(img_path)
    prediction = model.predict(img_array)
    class_idx = np.argmax(prediction)  # Obt√©m o √≠ndice da classe com maior probabilidade
    class_label = class_dict[class_idx]  # Obt√©m o r√≥tulo da classe

    # Exibe a imagem e a previs√£o
    plt.imshow(image.load_img(img_path))
    plt.axis('off')
    plt.title(f"Predi√ß√£o: {class_label}")
    plt.show()

    return class_label

# Exemplo de uso: carregue uma imagem do seu Colab e passe o caminho dela
img_path = "/content/drive/MyDrive/chest_xray_pequeno/train/NORMAL/person10_bacteria_43.jpeg"  # Substitua pelo caminho real
resultado = predict_image(img_path)
print("A imagem foi classificada como:", resultado)

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt

# Carrega o modelo treinado
model = load_model('model_densenet121.h5')

# Dicion√°rio de classes
class_dict = {0: "Normal", 1: "Pneumonia"}  # Ajuste conforme necess√°rio

def preprocess_image(img_path, target_size=(250, 250)):
    """Carrega e processa a imagem para o modelo."""
    img = image.load_img(img_path, target_size=target_size)
    img_array = image.img_to_array(img) / 255.0  # Normaliza a imagem
    img_array = np.expand_dims(img_array, axis=0)  # Adiciona dimens√£o do batch
    return img_array

def predict_image(img_path):
    """Classifica a imagem carregada e exibe as probabilidades."""
    img_array = preprocess_image(img_path)
    prediction = model.predict(img_array)[0]  # Obt√©m o valor da probabilidade (vetor de 1x1 para bin√°rio)

    # A probabilidade de ser Pneumonia (valores entre 0 e 1)
    prob_pneumonia = prediction[0] * 100  # Convertendo para porcentagem
    prob_normal = 100 - prob_pneumonia  # Como √© bin√°rio, a probabilidade de normal √© o complemento

    # √çndice e nome da classe com maior probabilidade
    class_idx = 1 if prob_pneumonia > 50 else 0  # Se a probabilidade for maior que 50% √© Pneumonia
    class_label = class_dict[class_idx]

    # Exibe a imagem e as probabilidades
    plt.imshow(image.load_img(img_path))
    plt.axis('off')
    plt.title(f"Predi√ß√£o: {class_label}\nNormal: {prob_normal:.2f}% | Pneumonia: {prob_pneumonia:.2f}%")
    plt.show()

    return class_label, prob_normal, prob_pneumonia

# Exemplo de uso
img_path = "/content/drive/MyDrive/chest_xray_pequeno/train/NORMAL/person10_bacteria_43.jpeg"  # Substitua pelo caminho real
resultado, prob_normal, prob_pneumonia = predict_image(img_path)

# Exibe as probabilidades no console
print(f"A imagem foi classificada como: {resultado}")
print(f"Probabilidade de ser Normal: {prob_normal:.2f}%")
print(f"Probabilidade de ser Pneumonia: {prob_pneumonia:.2f}%")

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt

# Carrega o modelo treinado
model = load_model('model_densenet121.h5')

# Dicion√°rio de classes
class_dict = {0: "Normal", 1: "Pneumonia"}  # Ajuste conforme necess√°rio

def preprocess_image(img_path, target_size=(250, 250)):
    """Carrega e processa a imagem para o modelo."""
    img = image.load_img(img_path, target_size=target_size)
    img_array = image.img_to_array(img) / 255.0  # Normaliza a imagem
    img_array = np.expand_dims(img_array, axis=0)  # Adiciona dimens√£o do batch
    return img_array

def predict_image(img_path):
    """Classifica a imagem carregada e exibe as probabilidades."""
    img_array = preprocess_image(img_path)
    prediction = model.predict(img_array)[0]  # Obt√©m os valores de sa√≠da da rede

    # Probabilidades das classes
    prob_normal = prediction[0] * 100  # Convertendo para porcentagem
    prob_pneumonia = prediction[1] * 100  # Convertendo para porcentagem

    # √çndice e nome da classe com maior probabilidade
    class_idx = np.argmax(prediction)
    class_label = class_dict[class_idx]

    # Exibe a imagem e as probabilidades
    plt.imshow(image.load_img(img_path))
    plt.axis('off')
    plt.title(f"Predi√ß√£o: {class_label}\nNormal: {prob_normal:.2f}% | Pneumonia: {prob_pneumonia:.2f}%")
    plt.show()

    return class_label, prob_normal, prob_pneumonia

# Exemplo de uso
img_path = "/content/drive/MyDrive/images/pulmao.jpeg"  # Substitua pelo caminho real
resultado, prob_normal, prob_pneumonia = predict_image(img_path)

# Exibe as probabilidades no console
print(f"A imagem foi classificada como: {resultado}")
print(f"Probabilidade de ser Normal: {prob_normal:.2f}%")
print(f"Probabilidade de ser Pneumonia: {prob_pneumonia:.2f}%")

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt

# Carrega o modelo treinado
model = load_model('model_densenet121.h5')

# Dicion√°rio de classes
class_dict = {0: "Normal", 1: "Pneumonia"}  # Ajuste conforme necess√°rio

def preprocess_image(img_path, target_size=(250, 250)):
    """Carrega e processa a imagem para o modelo."""
    img = image.load_img(img_path, target_size=target_size)
    img_array = image.img_to_array(img) / 255.0  # Normaliza a imagem
    img_array = np.expand_dims(img_array, axis=0)  # Adiciona dimens√£o do batch
    return img_array

def predict_image(img_path):
    """Classifica a imagem carregada e exibe as probabilidades."""
    img_array = preprocess_image(img_path)
    prediction = model.predict(img_array)[0]  # Obt√©m os valores de sa√≠da da rede

    # Probabilidades das classes
    prob_normal = prediction[0] * 100  # Convertendo para porcentagem
    prob_pneumonia = prediction[1] * 100  # Convertendo para porcentagem

    # √çndice e nome da classe com maior probabilidade
    class_idx = np.argmax(prediction)
    class_label = class_dict[class_idx]

    # Exibe a imagem e as probabilidades
    plt.imshow(image.load_img(img_path))
    plt.axis('off')
    plt.title(f"Predi√ß√£o: {class_label}\nNormal: {prob_normal:.2f}% | Pneumonia: {prob_pneumonia:.2f}%")
    plt.show()

    return class_label, prob_normal, prob_pneumonia

# Exemplo de uso
img_path = "/content/drive/MyDrive/images/pneumonia-bilateral.jpg"  # Substitua pelo caminho real
resultado, prob_normal, prob_pneumonia = predict_image(img_path)

# Exibe as probabilidades no console
print(f"A imagem foi classificada como: {resultado}")
print(f"Probabilidade de ser Normal: {prob_normal:.2f}%")
print(f"Probabilidade de ser Pneumonia: {prob_pneumonia:.2f}%")

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt

# Carrega o modelo treinado
model = load_model('model_densenet121.h5')

# Dicion√°rio de classes
class_dict = {0: "Normal", 1: "Pneumonia"}  # Ajuste conforme necess√°rio

def preprocess_image(img_path, target_size=(250, 250)):
    """Carrega e processa a imagem para o modelo."""
    img = image.load_img(img_path, target_size=target_size)
    img_array = image.img_to_array(img) / 255.0  # Normaliza a imagem
    img_array = np.expand_dims(img_array, axis=0)  # Adiciona dimens√£o do batch
    return img_array

def predict_image(img_path):
    """Classifica a imagem carregada e exibe as probabilidades."""
    img_array = preprocess_image(img_path)
    prediction = model.predict(img_array)[0]  # Obt√©m os valores de sa√≠da da rede

    # Probabilidades das classes
    prob_normal = prediction[0] * 100  # Convertendo para porcentagem
    prob_pneumonia = prediction[1] * 100  # Convertendo para porcentagem

    # √çndice e nome da classe com maior probabilidade
    class_idx = np.argmax(prediction)
    class_label = class_dict[class_idx]

    # Exibe a imagem e as probabilidades
    plt.imshow(image.load_img(img_path))
    plt.axis('off')
    plt.title(f"Predi√ß√£o: {class_label}\nNormal: {prob_normal:.2f}% | Pneumonia: {prob_pneumonia:.2f}%")
    plt.show()

    return class_label, prob_normal, prob_pneumonia

# Exemplo de uso
img_path = "/content/drive/MyDrive/images/abcesso-pulmonar.jpg"  # Substitua pelo caminho real
resultado, prob_normal, prob_pneumonia = predict_image(img_path)

# Exibe as probabilidades no console
print(f"A imagem foi classificada como: {resultado}")
print(f"Probabilidade de ser Normal: {prob_normal:.2f}%")
print(f"Probabilidade de ser Pneumonia: {prob_pneumonia:.2f}%")

from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# üîπ Defini√ß√£o do modelo DenseNet121
base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(250, 250, 3))

model = Sequential([
    base_model,
    Flatten(),
    Dense(256, activation='relu'),
    Dense(2, activation='softmax')  # üîÑ 2 classes
])

# üîπ Plotar o modelo e salvar como imagem
plot_model(model, to_file='densenet121_model.png', show_shapes=True, show_layer_names=True)

from tensorflow.keras.utils import plot_model
plot_model(model, to_file='vgg16_model.png', show_shapes=True, show_layer_names=True)



import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt

# Carrega o modelo treinado
model = load_model('modelvgg16.h5')

# Dicion√°rio de classes
class_dict = {0: "Normal", 1: "Pneumonia"}  # Ajuste conforme necess√°rio

def preprocess_image(img_path, target_size=(250, 250)):
    """Carrega e processa a imagem para o modelo."""
    img = image.load_img(img_path, target_size=target_size)
    img_array = image.img_to_array(img) / 255.0  # Normaliza a imagem
    img_array = np.expand_dims(img_array, axis=0)  # Adiciona dimens√£o do batch
    return img_array

def predict_image(img_path):
    """Classifica a imagem carregada e exibe as probabilidades."""
    img_array = preprocess_image(img_path)
    prediction = model.predict(img_array)[0]  # Obt√©m os valores de sa√≠da da rede

    # Probabilidades das classes
    prob_normal = prediction[0] * 100  # Convertendo para porcentagem
    prob_pneumonia = prediction[1] * 100  # Convertendo para porcentagem

    # √çndice e nome da classe com maior probabilidade
    class_idx = np.argmax(prediction)
    class_label = class_dict[class_idx]

    # Exibe a imagem e as probabilidades
    plt.imshow(image.load_img(img_path))
    plt.axis('off')
    plt.title(f"Predi√ß√£o: {class_label}\nNormal: {prob_normal:.2f}% | Pneumonia: {prob_pneumonia:.2f}%")
    plt.show()

    return class_label, prob_normal, prob_pneumonia

# Exemplo de uso
img_path = "/content/drive/MyDrive/images/b9dc356e-pneumonia-3.jpg"  # Substitua pelo caminho real
resultado, prob_normal, prob_pneumonia = predict_image(img_path)

# Exibe as probabilidades no console
print(f"A imagem foi classificada como: {resultado}")
print(f"Probabilidade de ser Normal: {prob_normal:.2f}%")
print(f"Probabilidade de ser Pneumonia: {prob_pneumonia:.2f}%")

"""# **Modelo keras.applications DenseNet201**"""

from tensorflow.keras.applications import DenseNet201
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# Carregar o modelo pr√©-treinado sem a √∫ltima camada
base_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(250, 250, 3))

# Criar um novo modelo com camadas extras
model = Sequential([
    base_model,
    Flatten(),
    Dense(256, activation='relu'),
    Dense(2, activation='softmax')  # üîÑ Multiclasse
])

# Congelar as camadas pr√©-treinadas
for layer in base_model.layers:
    layer.trainable = False

# Compilar para classifica√ß√£o multiclasse
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Resumo do modelo
model.summary()

from tensorflow import keras

# Definindo o n√∫mero de √©pocas e o batch size
epochs = 10

# Callback para salvar o melhor modelo
callbacks_list = [
    keras.callbacks.ModelCheckpoint(
        filepath='model_densenet201.h5',  # üîÑ Nome atualizado
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        verbose=1
    )
]

# üöÄ Treinamento
history = model.fit(
    train_generator,  # Gerador de dados de treino
    steps_per_epoch=nb_train_samples // BATCH_SIZE,
    epochs=epochs,
    callbacks=callbacks_list,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // BATCH_SIZE,
    verbose=1
)

# Training curves
import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

epochs_x = range(1, len(loss_values) + 1)
plt.figure(figsize=(10,10))
plt.subplot(2,1,1)
plt.plot(epochs_x, loss_values, 'bo', label='Training loss')
plt.plot(epochs_x, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation Loss and Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.subplot(2,1,2)
acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']
plt.plot(epochs_x, acc_values, 'bo', label='Training acc')
plt.plot(epochs_x, val_acc_values, 'b', label='Validation acc')
#plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend()
plt.show()

# Load the best saved model
from tensorflow.keras.models import load_model

model = load_model('model_densenet201.h5')

# ‚úÖ Avaliando com o dataset de valida√ß√£o
score = model.evaluate(validation_generator)
print('Val loss:', score[0])
print('Val accuracy:', score[1])

# ‚úÖ Avaliando com o dataset de teste

score = model.evaluate(test_generator)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

import numpy as np
from sklearn.metrics import accuracy_score, roc_auc_score

# üîπ Previs√µes do modelo (probabilidades)
predictions = model.predict(test_generator, verbose=1)

# üîπ R√≥tulos reais (inteiros)
true_labels = test_generator.classes

# üîπ Calcular acur√°cia (usando classe com maior probabilidade)
accuracy = accuracy_score(true_labels, np.argmax(predictions, axis=1))
print(f"Test Accuracy: {accuracy:.4f}")

# üîπ Calcular AUC
# Para 2 classes, usa-se multi_class='ovr'
auc = roc_auc_score(true_labels, predictions[:,1], multi_class='ovr')
print(f"Test AUC: {auc:.4f}")

# Criar gerador de teste corretamente
test_generator = test_datagen.flow_from_directory(
    TEST_DIR,
    target_size=(250, 250),
    batch_size=BATCH_SIZE,
    class_mode='categorical',  # compat√≠vel com softmax
    shuffle=False               # importante para avalia√ß√£o
)

# Avaliar o modelo
score = model.evaluate(
    test_generator,
    steps=math.ceil(nb_test_samples / BATCH_SIZE),  # garante que todas as imagens sejam avaliadas
    verbose=1
)
print(f'Test Loss: {score[0]:.4f}, Test Accuracy: {score[1]:.4f}')


score = model.evaluate(test_generator)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# Some reports
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# On test dataset
Y_pred = model.predict(test_generator)
y_pred = np.argmax(Y_pred, axis=1)
target_names = classes

# Confusion Matrix
cm = confusion_matrix(test_generator.classes, y_pred)
plot_confusion_matrix(cm, target_names, normalize=False, title='Confusion Matrix')

# Classification Report
print('Classification Report')
print(classification_report(test_generator.classes, y_pred, target_names=target_names))

"""# **Modelo keras.applications ResNet50**"""

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# üîπ Carregar o modelo pr√©-treinado sem a √∫ltima camada
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(250, 250, 3))

# üîπ Criar um novo modelo com camadas extras
model = Sequential([
    base_model,
    Flatten(),
    Dense(256, activation='relu'),
    Dense(2, activation='softmax')  # üîÑ Multiclasse
])

# üîπ Congelar as camadas pr√©-treinadas
for layer in base_model.layers:
    layer.trainable = False

# üîπ Compilar para classifica√ß√£o multiclasse
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# üîπ Resumo do modelo
model.summary()

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow import keras

# üîπ Defini√ß√£o do modelo ResNet50
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(250, 250, 3))

model = Sequential([
    base_model,
    Flatten(),
    Dense(256, activation='relu'),
    Dense(2, activation='softmax')  # üîÑ 2 classes com softmax
])

# üîπ Congelar camadas pr√©-treinadas
for layer in base_model.layers:
    layer.trainable = False

# üîπ Compilar
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# üîπ Definindo o n√∫mero de √©pocas e o batch size
epochs = 10

# üîπ Callback para salvar o melhor modelo
callbacks_list = [
    keras.callbacks.ModelCheckpoint(
        filepath='model_resnet50.h5',  # ‚úÖ Nome atualizado
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        verbose=1
    )
]

# üöÄ Treinamento
history = model.fit(
    train_generator,  # Gerador de dados de treino
    steps_per_epoch=nb_train_samples // BATCH_SIZE,
    epochs=epochs,
    callbacks=callbacks_list,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // BATCH_SIZE,
    verbose=1
)

# Training curves
import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

epochs_x = range(1, len(loss_values) + 1)
plt.figure(figsize=(10,10))
plt.subplot(2,1,1)
plt.plot(epochs_x, loss_values, 'bo', label='Training loss')
plt.plot(epochs_x, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation Loss and Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.subplot(2,1,2)
acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']
plt.plot(epochs_x, acc_values, 'bo', label='Training acc')
plt.plot(epochs_x, val_acc_values, 'b', label='Validation acc')
#plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend()
plt.show()

# Load the best saved model
from tensorflow.keras.models import load_model

model = load_model('model_resnet50.h5')

# ‚úÖ Avaliando com o dataset de valida√ß√£o
score = model.evaluate(validation_generator)
print('Val loss:', score[0])
print('Val accuracy:', score[1])

import numpy as np
from sklearn.metrics import accuracy_score, roc_auc_score

# üîπ Previs√µes do modelo (probabilidades)
predictions = model.predict(test_generator, verbose=1)

# üîπ R√≥tulos reais (inteiros)
true_labels = test_generator.classes

# üîπ Calcular acur√°cia (usando classe com maior probabilidade)
accuracy = accuracy_score(true_labels, np.argmax(predictions, axis=1))
print(f"Test Accuracy: {accuracy:.4f}")

# üîπ Calcular AUC
# Para 2 classes, usa-se multi_class='ovr'
auc = roc_auc_score(true_labels, predictions[:,1], multi_class='ovr')
print(f"Test AUC: {auc:.4f}")

# Relat√≥rios
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# Previs√µes
Y_pred = model.predict(test_generator)
y_pred = np.argmax(Y_pred, axis=1)

# Nomes das classes (pegando direto do generator)
target_names = list(test_generator.class_indices.keys())

# Matriz de confus√£o
cm = confusion_matrix(test_generator.classes, y_pred)

# Plot da matriz de confus√£o
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Classification Report
print("Classification Report")
print(classification_report(test_generator.classes, y_pred, target_names=target_names))

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

# 1) Gerar previs√µes no conjunto de teste
# Se voc√™ j√° tiver y_pred pronto, pode pular esta parte
Y_pred = model.predict(test_generator, verbose=1)
y_pred = np.argmax(Y_pred, axis=1)  # classes previstas

# 2) Distribui√ß√£o das previs√µes
unique, counts = np.unique(y_pred, return_counts=True)
print("Distribui√ß√£o de previs√µes:", dict(zip(unique, counts)))

# 3) Relat√≥rio de classifica√ß√£o (sem warnings)
print(classification_report(
    test_generator.classes,   # classes verdadeiras
    y_pred,                   # classes previstas
    target_names=list(test_generator.class_indices.keys()),
    zero_division=0           # evita warnings, coloca 0.0 onde n√£o h√° previs√£o
))

# 4) (Opcional) Matriz de confus√£o
cm = confusion_matrix(test_generator.classes, y_pred)
print("Matriz de confus√£o:\n", cm)

# Plot da matriz de confus√£o
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Classification Report
print("Classification Report")
print(classification_report(test_generator.classes, y_pred, target_names=target_names))

"""# **Modelo keras.applications ResNet50**"""

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D

# üîπ Carregar o modelo pr√©-treinado sem a √∫ltima camada
base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(250, 250, 3))

# üîπ Criar um novo modelo com camadas extras
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),  # üîÑ substitui Flatten() (mais eficiente para EfficientNet)
    Dense(256, activation='relu'),
    Dense(2, activation='softmax')  # üîÑ Multiclasse
])

# üîπ Congelar as camadas pr√©-treinadas
for layer in base_model.layers:
    layer.trainable = False

# üîπ Compilar para classifica√ß√£o multiclasse
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# üîπ Resumo do modelo
model.summary()

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow import keras

# üîπ Defini√ß√£o do modelo EfficientNetB0
base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(250, 250, 3))

model = Sequential([
    base_model,
    Flatten(),
    Dense(256, activation='relu'),
    Dense(2, activation='softmax')  # üîÑ 2 classes com softmax
])

# üîπ Congelar camadas pr√©-treinadas
for layer in base_model.layers:
    layer.trainable = False

# üîπ Compilar
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# üîπ Definindo o n√∫mero de √©pocas e o batch size
epochs = 10

# üîπ Callback para salvar o melhor modelo
callbacks_list = [
    keras.callbacks.ModelCheckpoint(
        filepath='model_efficientnetb0.h5',  # ‚úÖ Nome atualizado
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        verbose=1
    )
]

# üöÄ Treinamento
history = model.fit(
    train_generator,  # Gerador de dados de treino
    steps_per_epoch=nb_train_samples // BATCH_SIZE,
    epochs=epochs,
    callbacks=callbacks_list,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // BATCH_SIZE,
    verbose=1
)

# Training curves
import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

epochs_x = range(1, len(loss_values) + 1)
plt.figure(figsize=(10,10))
plt.subplot(2,1,1)
plt.plot(epochs_x, loss_values, 'bo', label='Training loss')
plt.plot(epochs_x, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation Loss and Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.subplot(2,1,2)
acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']
plt.plot(epochs_x, acc_values, 'bo', label='Training acc')
plt.plot(epochs_x, val_acc_values, 'b', label='Validation acc')
#plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend()
plt.show()

# Load the best saved model
from tensorflow.keras.models import load_model

model = load_model('model_efficientnetb0.h5')

# ‚úÖ Avaliando com o dataset de valida√ß√£o
score = model.evaluate(validation_generator)
print('Val loss:', score[0])
print('Val accuracy:', score[1])

import numpy as np
from sklearn.metrics import accuracy_score, roc_auc_score

# üîπ Previs√µes do modelo (probabilidades)
predictions = model.predict(test_generator, verbose=1)

# üîπ R√≥tulos reais (inteiros)
true_labels = test_generator.classes

# üîπ Calcular acur√°cia (usando classe com maior probabilidade)
accuracy = accuracy_score(true_labels, np.argmax(predictions, axis=1))
print(f"Test Accuracy: {accuracy:.4f}")

# üîπ Calcular AUC
# Para 2 classes, usa-se multi_class='ovr'
auc = roc_auc_score(true_labels, predictions[:,1], multi_class='ovr')
print(f"Test AUC: {auc:.4f}")

# Load the best saved model
from tensorflow.keras.models import load_model

model = load_model('model_resnet50.h5')

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

# 1) Gerar previs√µes no conjunto de teste
# Se voc√™ j√° tiver y_pred pronto, pode pular esta parte
Y_pred = model.predict(test_generator, verbose=1)
y_pred = np.argmax(Y_pred, axis=1)  # classes previstas

# 2) Distribui√ß√£o das previs√µes
unique, counts = np.unique(y_pred, return_counts=True)
print("Distribui√ß√£o de previs√µes:", dict(zip(unique, counts)))

# 3) Relat√≥rio de classifica√ß√£o (sem warnings)
print(classification_report(
    test_generator.classes,   # classes verdadeiras
    y_pred,                   # classes previstas
    target_names=list(test_generator.class_indices.keys()),
    zero_division=0           # evita warnings, coloca 0.0 onde n√£o h√° previs√£o
))

# 4) (Opcional) Matriz de confus√£o
cm = confusion_matrix(test_generator.classes, y_pred)
print("Matriz de confus√£o:\n", cm)

# ‚úÖ Modelo com EfficientNetB0
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow import keras

# üîπ Carregar EfficientNetB0 pr√©-treinada
base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(250, 250, 3))

# üîπ Criar modelo sequencial
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),  # üîÑ melhor que Flatten para EfficientNet
    Dense(256, activation='relu'),
    Dense(2, activation='softmax')  # üîÑ 2 classes com softmax
])

# üîπ Congelar camadas da EfficientNet
for layer in base_model.layers:
    layer.trainable = False

# üîπ Compilar modelo
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# üîπ Defini√ß√£o de par√¢metros
epochs = 10

# üîπ Callbacks
callbacks_list = [
    keras.callbacks.ModelCheckpoint(
        filepath='model_efficientnetb0.h5',
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        verbose=1
    )
]

# üöÄ Treinamento
history = model.fit(
    train_generator,
    steps_per_epoch=nb_train_samples // BATCH_SIZE,
    epochs=epochs,
    callbacks=callbacks_list,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // BATCH_SIZE,
    verbose=1
)

# üìâ Curvas de treinamento
import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']

epochs_x = range(1, len(loss_values) + 1)
plt.figure(figsize=(10,10))

plt.subplot(2,1,1)
plt.plot(epochs_x, loss_values, 'bo', label='Training loss')
plt.plot(epochs_x, val_loss_values, 'b', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(2,1,2)
plt.plot(epochs_x, acc_values, 'bo', label='Training acc')
plt.plot(epochs_x, val_acc_values, 'b', label='Validation acc')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

# ‚úÖ Carregar melhor modelo salvo
from tensorflow.keras.models import load_model
model = load_model('model_efficientnetb0.h5')

# üîπ Avalia√ß√£o no conjunto de valida√ß√£o
score = model.evaluate(validation_generator)
print('Val loss:', score[0])
print('Val accuracy:', score[1])

# ‚úÖ Teste com m√©tricas extras
import numpy as np
from sklearn.metrics import accuracy_score, roc_auc_score

# Previs√µes no teste
predictions = model.predict(test_generator, verbose=1)
true_labels = test_generator.classes

# Acur√°cia
accuracy = accuracy_score(true_labels, np.argmax(predictions, axis=1))
print(f"Test Accuracy: {accuracy:.4f}")

# AUC
auc = roc_auc_score(true_labels, predictions[:,1])
print(f"Test AUC: {auc:.4f}")

# ‚úÖ Relat√≥rios
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

Y_pred = model.predict(test_generator)
y_pred = np.argmax(Y_pred, axis=1)

# Nomes das classes
target_names = list(test_generator.class_indices.keys())

# Matriz de confus√£o
cm = confusion_matrix(test_generator.classes, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Classification Report
print("Classification Report")
print(classification_report(test_generator.classes, y_pred, target_names=target_names, zero_division=0))



# ‚úÖ Modelo com ResNet50
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.models import load_model

# üîπ Carregar ResNet50 pr√©-treinada
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(250, 250, 3))

# üîπ Criar modelo sequencial
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),  # üîÑ mais eficiente que Flatten
    Dense(256, activation='relu'),
    Dense(2, activation='softmax')  # üîÑ 2 classes com softmax
])

# üîπ Congelar camadas da ResNet50
for layer in base_model.layers:
    layer.trainable = False

# üîπ Compilar modelo
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# üîπ Defini√ß√£o de par√¢metros
epochs = 10

# üîπ Callbacks
callbacks_list = [
    keras.callbacks.ModelCheckpoint(
        filepath='model_resnet50.h5',
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        verbose=1
    )
]

# üöÄ Treinamento
history = model.fit(
    train_generator,
    steps_per_epoch=nb_train_samples // BATCH_SIZE,
    epochs=epochs,
    callbacks=callbacks_list,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // BATCH_SIZE,
    verbose=1
)

# üìâ Curvas de treinamento
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']

epochs_x = range(1, len(loss_values) + 1)
plt.figure(figsize=(10,10))

plt.subplot(2,1,1)
plt.plot(epochs_x, loss_values, 'bo', label='Training loss')
plt.plot(epochs_x, val_loss_values, 'b', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(2,1,2)
plt.plot(epochs_x, acc_values, 'bo', label='Training acc')
plt.plot(epochs_x, val_acc_values, 'b', label='Validation acc')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# ‚úÖ Carregar melhor modelo salvo
model = load_model('model_resnet50.h5')

# üîπ Avalia√ß√£o no conjunto de valida√ß√£o
score = model.evaluate(validation_generator)
print('Val loss:', score[0])
print('Val accuracy:', score[1])

# ‚úÖ Teste com m√©tricas extras
predictions = model.predict(test_generator, verbose=1)
true_labels = test_generator.classes

# Acur√°cia
accuracy = accuracy_score(true_labels, np.argmax(predictions, axis=1))
print(f"Test Accuracy: {accuracy:.4f}")

# AUC
auc = roc_auc_score(true_labels, predictions[:,1])
print(f"Test AUC: {auc:.4f}")

# ‚úÖ Relat√≥rios
Y_pred = model.predict(test_generator)
y_pred = np.argmax(Y_pred, axis=1)
target_names = list(test_generator.class_indices.keys())

# Matriz de confus√£o
cm = confusion_matrix(test_generator.classes, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Classification Report
print("Classification Report")
print(classification_report(test_generator.classes, y_pred, target_names=target_names, zero_division=0))



